@inproceedings{9799968,
  title = {Classify Bird Species Audio by Augment Convolutional Neural Network},
  booktitle = {2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications ({{HORA}})},
  author = {Jasim, Hasan Abdullah and Ahmed, Saadaldeen R. and Ibrahim, Abdullahi Abdu and Duru, Adil Deniz},
  year = {2022},
  pages = {1--6},
  doi = {10.1109/HORA55278.2022.9799968},
  keywords = {audio,Birds,classification,CNN,Convolutional neural networks,Feature extraction,identification,Kernel,librosa,Solids,spectrogram,Spectrogram,Training,xeno-canto}
}

@inproceedings{abadiTensorFlowSystemLargeScale2016,
  title = {\{\vphantom\}{{TensorFlow}}\vphantom\{\}: {{A System}} for \{\vphantom\}{{Large-Scale}}\vphantom\{\} {{Machine Learning}}},
  shorttitle = {\{\vphantom\}{{TensorFlow}}\vphantom\{\}},
  booktitle = {12th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 16)},
  author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  pages = {265--283},
  urldate = {2024-12-16},
  isbn = {978-1-931971-33-1},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\SEDIJKSE\Abadi et al. - 2016 - TensorFlow A System for Large-Scale Machine L.pdf}
}

@article{amesoderCitoPackageTraining2024,
  title = {`cito': An {{R}} Package for Training Neural Networks Using `torch'},
  shorttitle = {`cito'},
  author = {Ames{\"o}der, Christian and Hartig, Florian and Pichler, Maximilian},
  year = {2024},
  month = jun,
  journal = {Ecography},
  volume = {2024},
  number = {6},
  pages = {e07143},
  issn = {0906-7590, 1600-0587},
  doi = {10.1111/ecog.07143},
  urldate = {2024-11-19},
  abstract = {Deep neural networks (DNN) have become a central method in ecology. To build and train DNNs in deep learning (DL) applications, most users rely on one of the major deep learning frameworks, in particular PyTorch or TensorFlow. Using these frameworks, however, requires substantial experience and time. Here, we present `cito', a user-friendly R package for DL that allows specifying DNNs in the familiar formula syntax used by many R packages. To fit the models, `cito' takes advantage of the numerically optimized `torch' library, including the ability to switch between training models on the CPU or the graphics processing unit (GPU) which allows the efficient training of large DNNs. Moreover, `cito' includes many user-friendly functions for model plotting and analysis, including explainable AI (xAI) metrics for effect sizes and variable importance. All xAI metrics as well as predictions can optionally be bootstrapped to generate confidence intervals, including p-values. To showcase a typical analysis pipeline using `cito', with its built-in xAI features, we built a species distribution model of the African elephant. We hope that by providing a user-friendly R framework to specify, deploy and interpret DNNs, `cito' will make this interesting class of models more accessible to ecological data analysis. A stable version of `cito' can be installed from the comprehensive R archive network (CRAN).},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\B9Q2H7E4\Amesöder et al. - 2024 - ‘cito' an R package for training neural networks .pdf}
}

@misc{amesoederCitoPackageTraining2023,
  title = {Cito: {{An R}} Package for Training Neural Networks Using Torch},
  shorttitle = {Cito},
  author = {Amesoeder, Christian and Hartig, Florian and Pichler, Maximilian},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09599},
  eprint = {2303.09599},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09599},
  urldate = {2023-04-20},
  abstract = {1. Deep neural networks (DNN) have become a central class of algorithms for regression and classification tasks. Although some packages exist that allow users to specify DNN in R, those are rather limited in their functionality. Most current deep learning applications therefore rely on one of the major deep learning frameworks, PyTorch or TensorFlow, to build and train DNN. However, using these frameworks requires substantially more training and time than comparable regression or machine learning packages in the R environment. 2. Here, we present cito, an user-friendly R package for deep learning. cito allows R users to specify deep neural networks in the familiar formula syntax used by most modeling functions in R. In the background, cito uses torch to fit the models, taking advantage of all the numerical optimizations of the torch library, including the ability to switch between training models on CPUs or GPUs. Moreover, cito includes many user-friendly functions for predictions and an explainable Artificial Intelligence (xAI) pipeline for the fitted models. 3. We showcase a typical analysis pipeline using cito, including its built-in xAI features to explore the trained DNN, by building a species distribution model of the African elephant. 4. In conclusion, cito provides a user-friendly R framework to specify, deploy and interpret deep neural networks based on torch. The current stable CRAN version mainly supports fully connected DNNs, but it is planned that future versions will also include CNNs and RNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.5.4},
  file = {C\:\\Users\\armin\\Zotero\\storage\\IGIATVHA\\Amesoeder et al. - 2023 - cito An R package for training neural networks us.pdf;C\:\\Users\\armin\\Zotero\\storage\\3VYY5LSX\\2303.html}
}

@article{anandIntegratingMultiSensorsData2021,
  title = {Integrating {{Multi-Sensors Data}} for {{Species Distribution Mapping Using Deep Learning}} and {{Envelope Models}}},
  author = {Anand, Akash and Pandey, Manish K. and Srivastava, Prashant K. and Gupta, Ayushi and Khan, Mohammed Latif},
  year = {2021},
  month = jan,
  journal = {Remote Sensing},
  volume = {13},
  number = {16},
  pages = {3284},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs13163284},
  urldate = {2023-04-26},
  abstract = {The integration of ecological and atmospheric characteristics for biodiversity management is fundamental for long-term ecosystem conservation and drafting forest management strategies, especially in the current era of climate change. The explicit modelling of regional ecological responses and their impact on individual species is a significant prerequisite for any adaptation strategy. The present study focuses on predicting the regional distribution of Rhododendron arboreum, a medicinal plant species found in the Himalayan region. Advanced Species Distribution Models (SDM) based on the principle of predefined hypothesis, namely BIOCLIM, was used to model the potential distribution of Rhododendron arboreum. This hypothesis tends to vary with the change in locations, and thus, robust models are required to establish nonlinear complex relations between the input parameters. To address this nonlinear relation, a class of deep neural networks, Convolutional Neural Network (CNN) architecture is proposed, designed, and tested, which eventually gave much better accuracy than the BIOCLIM model. Both of the models were given 16 input parameters, including ecological and atmospheric variables, which were statistically resampled and were then utilized in establishing the linear and nonlinear relationship to better fit the occurrence scenarios of the species. The input parameters were mostly acquired from the recent satellite missions, including MODIS, Sentinel-2, Sentinel-5p, the Shuttle Radar Topography Mission (SRTM), and ECOSTRESS. The performance across all the thresholds was evaluated using the value of the Area Under Curve (AUC) evaluation metrics. The AUC value was found to be 0.917 with CNN, whereas it was 0.68 with BIOCLIM, respectively. The performance evaluation metrics indicate the superiority of CNN for species distribution over BIOCLIM.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {<i>Rhododendron arboreum</i>,biodiversity management,convolutional neural network,ecological responses,spatial distribution modelling},
  file = {C:\Users\armin\Zotero\storage\6JCMISVV\Anand et al. - 2021 - Integrating Multi-Sensors Data for Species Distrib.pdf}
}

@article{asnerMappedAbovegroundCarbon2018,
  title = {Mapped Aboveground Carbon Stocks to Advance Forest Conservation and Recovery in {{Malaysian Borneo}}},
  author = {Asner, Gregory P. and Brodrick, Philip G. and Philipson, Christopher and Vaughn, Nicolas R. and Martin, Roberta E. and Knapp, David E. and Heckler, Joseph and Evans, Luke J. and Jucker, Tommaso and Goossens, Benoit and Stark, Danica J. and Reynolds, Glen and Ong, Robert and Renneboog, Nathan and Kugan, Fred and Coomes, David A.},
  year = {2018},
  month = jan,
  journal = {Biological Conservation},
  volume = {217},
  pages = {289--310},
  issn = {0006-3207},
  doi = {10.1016/j.biocon.2017.10.020},
  urldate = {2024-12-18},
  abstract = {Forest carbon stocks in rapidly developing tropical regions are highly heterogeneous, which challenges efforts to develop spatially-explicit conservation actions. In addition to field-based biodiversity information, mapping of carbon stocks can greatly accelerate the identification, protection and recovery of forests deemed to be of high conservation value (HCV). We combined airborne Light Detection and Ranging (LiDAR) with satellite imaging and other geospatial data to map forest aboveground carbon density at 30m (0.09ha) resolution throughout the Malaysian state of Sabah on the island of Borneo. We used the mapping results to assess how carbon stocks vary spatially based on forest use, deforestation, regrowth, and current forest protections. We found that unlogged, intact forests contain aboveground carbon densities averaging over 200MgCha-1, with peaks of 500MgCha-1. Critically, more than 40\% of the highest carbon stock forests were discovered outside of areas designated for maximum protection. Previously logged forests have suppressed, but still high, carbon densities of 60--140MgCha-1. Our mapped distributions of forest carbon stock suggest that the state of Sabah could double its total aboveground carbon storage if previously logged forests are allowed to recover in the future. Our results guide ongoing efforts to identify HCV forests and to determine new areas for forest protection in Borneo.},
  keywords = {Borneo,Carbon conservation,Carnegie airborne observatory,Deforestation,Forest conservation,Land use history,Sabah,Selective logging},
  file = {C\:\\Users\\armin\\Zotero\\storage\\KU6RDSDA\\Asner et al. - 2018 - Mapped aboveground carbon stocks to advance forest.pdf;C\:\\Users\\armin\\Zotero\\storage\\2S68GIP5\\S0006320717310790.html}
}

@article{bachPixelWiseExplanationsNonLinear2015,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2023-06-14},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Neural networks,Neurons,Sensory perception,Vision},
  file = {C:\Users\armin\Zotero\storage\TV9ZANLS\Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf}
}

@article{bachPixelWiseExplanationsNonLinear2015a,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2024-12-23},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Neural networks,Neurons,Sensory perception,Vision},
  file = {C:\Users\armin\Zotero\storage\CKKVRRDP\Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf}
}

@article{batesFittingLinearMixedeffects2015,
  title = {Fitting Linear Mixed-Effects Models Using {{lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01}
}

@misc{birdclef-2024,
  title = {{{BirdCLEF}} 2024},
  author = {Klinck, Holger and {Maggie} and Dane, Sohier and Kahl, Stefan and Denton, Tom and Ramesh, Vijay},
  year = {2024}
}

@misc{BirdCLEF2024,
  title = {{{BirdCLEF}} 2024},
  urldate = {2024-12-10},
  abstract = {Bird species identification from audio, focused on under-studied species in the Western Ghats, a major biodiversity hotspot in India.},
  howpublished = {https://kaggle.com/birdclef-2024},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\V7YJXQSQ\citation.html}
}

@misc{bojarskiEndEndLearning2016,
  title = {End to {{End Learning}} for {{Self-Driving Cars}}},
  author = {Bojarski, Mariusz and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  year = {2016},
  month = apr,
  number = {arXiv:1604.07316},
  eprint = {1604.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1604.07316},
  urldate = {2024-12-18},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\armin\\Zotero\\storage\\7KN223Y4\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf;C\:\\Users\\armin\\Zotero\\storage\\V3RFH82K\\1604.html}
}

@article{bold2019cross,
  title = {Cross-Domain Deep Feature Combination for Bird Species Classification with Audio-Visual Data},
  author = {Bold, Naranchimeg and Zhang, Chao and Akashi, Takuya},
  year = {2019},
  journal = {IEICE TRANSACTIONS on Information and Systems},
  volume = {102},
  number = {10},
  pages = {2033--2042},
  publisher = {{The Institute of Electronics, Information and Communication Engineers}}
}

@article{borowiecDeepLearningTool2022,
  title = {Deep Learning as a Tool for Ecology and Evolution},
  author = {Borowiec, Marek L. and Dikow, Rebecca B. and Frandsen, Paul B. and McKeeken, Alexander and Valentini, Gabriele and White, Alexander E.},
  year = {2022},
  journal = {Methods in Ecology and Evolution},
  volume = {13},
  number = {8},
  pages = {1640--1660},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13901},
  urldate = {2024-12-16},
  abstract = {Deep learning is driving recent advances behind many everyday technologies, including speech and image recognition, natural language processing and autonomous driving. It is also gaining popularity in biology, where it has been used for automated species identification, environmental monitoring, ecological modelling, behavioural studies, DNA sequencing and population genetics and phylogenetics, among other applications. Deep learning relies on artificial neural networks for predictive modelling and excels at recognizing complex patterns. In this review we synthesize 818 studies using deep learning in the context of ecology and evolution to give a discipline-wide perspective necessary to promote a rethinking of inference approaches in the field. We provide an introduction to machine learning and contrast it with mechanistic inference, followed by a gentle primer on deep learning. We review the applications of deep learning in ecology and evolution and discuss its limitations and efforts to overcome them. We also provide a practical primer for biologists interested in including deep learning in their toolkit and identify its possible future applications. We find that deep learning is being rapidly adopted in ecology and evolution, with 589 studies (64\%) published since the beginning of 2019. Most use convolutional neural networks (496 studies) and supervised learning for image identification but also for tasks using molecular data, sounds, environmental data or video as input. More sophisticated uses of deep learning in biology are also beginning to appear. Operating within the machine learning paradigm, deep learning can be viewed as an alternative to mechanistic modelling. It has desirable properties of good performance and scaling with increasing complexity, while posing unique challenges such as sensitivity to bias in input data. We expect that rapid adoption of deep learning in ecology and evolution will continue, especially in automation of biodiversity monitoring and discovery and inference from genetic data. Increased use of unsupervised learning for discovery and visualization of clusters and gaps, simplification of multi-step analysis pipelines, and integration of machine learning into graduate and postgraduate training are all likely in the near future.},
  copyright = {{\copyright} 2022 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {artificial intelligence,automation,computer vision,machine learning,modelling,neural networks,statistics},
  file = {C:\Users\armin\Zotero\storage\DUZ9IMPY\Borowiec et al. - 2022 - Deep learning as a tool for ecology and evolution.pdf}
}

@article{botella2018deep,
  title = {A Deep Learning Approach to Species Distribution Modelling},
  author = {Botella, Christophe and Joly, Alexis and Bonnet, Pierre and Monestiez, Pascal and Munoz, Fran{\c c}ois},
  year = {2018},
  journal = {Multimedia Tools and Applications for Environmental \& Biodiversity Informatics},
  pages = {169--199},
  publisher = {Springer}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-16},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {C:\Users\armin\Zotero\storage\ZKWK5ASB\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@misc{chenEndEndLearningDeep2018,
  title = {End-to-{{End Learning}} for the {{Deep Multivariate Probit Model}}},
  author = {Chen, Di and Xue, Yexiang and Gomes, Carla P.},
  year = {2018},
  month = jul,
  number = {arXiv:1803.08591},
  eprint = {1803.08591},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.08591},
  urldate = {2024-12-18},
  abstract = {The multivariate probit model (MVP) is a popular classic model for studying binary responses of multiple entities. Nevertheless, the computational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent variables, significantly limits its application in practice. We propose a flexible deep generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit GPU-boosted deep neural networks. We present both theoretical and empirical analysis of the convergence behavior of DMVP's sampling process with respect to the resolution of the correlation structure. We provide convergence guarantees for DMVP and our empirical analysis demonstrates the advantages of DMVP's sampling compared with standard MCMC-based methods. We also show that when applied to multi-entity modelling problems, which are natural DMVP applications, DMVP trains faster than classical MVP, by at least an order of magnitude, captures rich correlations among entities, and further improves the joint likelihood of entities compared with several competitive models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\FZEK637E\\Chen et al. - 2018 - End-to-End Learning for the Deep Multivariate Prob.pdf;C\:\\Users\\armin\\Zotero\\storage\\DTTLVLGE\\1803.html}
}

@misc{chollet2015keras,
  title = {Keras},
  author = {Chollet, Fran{\c c}ois and others},
  year = {2015}
}

@article{christinApplicationsDeepLearning2019,
  title = {Applications for Deep Learning in Ecology},
  author = {Christin, Sylvain and Hervet, {\'E}ric and Lecomte, Nicolas},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {10},
  pages = {1632--1644},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13256},
  urldate = {2024-12-16},
  abstract = {A lot of hype has recently been generated around deep learning, a novel group of artificial intelligence approaches able to break accuracy records in pattern recognition. Over the course of just a few years, deep learning has revolutionized several research fields such as bioinformatics and medicine with its flexibility and ability to process large and complex datasets. As ecological datasets are becoming larger and more complex, we believe these methods can be useful to ecologists as well. In this paper, we review existing implementations and show that deep learning has been used successfully to identify species, classify animal behaviour and estimate biodiversity in large datasets like camera-trap images, audio recordings and videos. We demonstrate that deep learning can be beneficial to most ecological disciplines, including applied contexts, such as management and conservation. We also identify common questions about how and when to use deep learning, such as what are the steps required to create a deep learning network, which tools are available to help, and what are the requirements in terms of data and computer power. We provide guidelines, recommendations and useful resources, including a reference flowchart to help ecologists get started with deep learning. We argue that at a time when automatic monitoring of populations and ecosystems generates a vast amount of data that cannot be effectively processed by humans anymore, deep learning could become a powerful reference tool for ecologists.},
  copyright = {{\copyright} 2019 The Authors. Methods in Ecology and Evolution {\copyright} 2019 British Ecological Society},
  langid = {english},
  keywords = {artificial intelligence,automatic monitoring,deep learning,ecology,neural network,pattern recognition},
  file = {C:\Users\armin\Zotero\storage\V5J77796\Christin et al. - 2019 - Applications for deep learning in ecology.pdf}
}

@misc{DeepLearningDetects,
  title = {Deep Learning Detects Invasive Plant Species across Complex Landscapes Using {{Worldview}}-2 and {{Planetscope}} Satellite Imagery},
  journal = {ResearchGate},
  urldate = {2024-12-18},
  abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
  howpublished = {https://www.researchgate.net/publication/361278783\_Deep\_learning\_detects\_invasive\_plant\_species\_across\_complex\_landscapes\_using\_Worldview-2\_and\_Planetscope\_satellite\_imagery},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\CMJHCNT4\361278783_Deep_learning_detects_invasive_plant_species_across_complex_landscapes_using_Worldvie.html}
}

@article{deneuConvolutionalNeuralNetworks2021,
  title = {Convolutional Neural Networks Improve Species Distribution Modelling by Capturing the Spatial Structure of the Environment},
  author = {Deneu, Benjamin and Servajean, Maximilien and Bonnet, Pierre and Botella, Christophe and Munoz, Fran{\c c}ois and Joly, Alexis},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008856},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008856},
  urldate = {2023-04-21},
  abstract = {Convolutional Neural Networks (CNNs) are statistical models suited for learning complex visual patterns. In the context of Species Distribution Models (SDM) and in line with predictions of landscape ecology and island biogeography, CNN could grasp how local landscape structure affects prediction of species occurrence in SDMs. The prediction can thus reflect the signatures of entangled ecological processes. Although previous machine-learning based SDMs can learn complex influences of environmental predictors, they cannot acknowledge the influence of environmental structure in local landscapes (hence denoted ``punctual models''). In this study, we applied CNNs to a large dataset of plant occurrences in France (GBIF), on a large taxonomical scale, to predict ranked relative probability of species (by joint learning) to any geographical position. We examined the way local environmental landscapes improve prediction by performing alternative CNN models deprived of information on landscape heterogeneity and structure (``ablation experiments''). We found that the landscape structure around location crucially contributed to improve predictive performance of CNN-SDMs. CNN models can classify the predicted distributions of many species, as other joint modelling approaches, but they further prove efficient in identifying the influence of local environmental landscapes. CNN can then represent signatures of spatially structured environmental drivers. The prediction gain is noticeable for rare species, which open promising perspectives for biodiversity monitoring and conservation strategies. Therefore, the approach is of both theoretical and practical interest. We discuss the way to test hypotheses on the patterns learnt by CNN, which should be essential for further interpretation of the ecological processes at play.},
  langid = {english},
  keywords = {Biogeography,Cartography,Decision trees,Ecological niches,Neural networks,Neurons,Statistical distributions,Theoretical ecology},
  file = {C:\Users\armin\Zotero\storage\N8MFZTU3\Deneu et al. - 2021 - Convolutional neural networks improve species dist.pdf}
}

@inproceedings{deneuEvaluationDeepSpecies2019,
  title = {Evaluation of {{Deep Species Distribution Models Using Environment}} and {{Co-occurrences}}},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Deneu, Benjamin and Servajean, Maximilien and Botella, Christophe and Joly, Alexis},
  editor = {Crestani, Fabio and Braschler, Martin and Savoy, Jacques and Rauber, Andreas and M{\"u}ller, Henning and Losada, David E. and Heinatz B{\"u}rki, Gundula and Cappellato, Linda and Ferro, Nicola},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {213--225},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-28577-7_18},
  abstract = {This paper presents an evaluation of several approaches of plants species distribution modeling based on spatial, environmental and co-occurrences data using machine learning methods. In particular, we re-evaluate the environmental convolutional neural network model that obtained the best performance of the GeoLifeCLEF 2018 challenge but on a revised dataset that fixes some of the issues of the previous one. We also go deeper in the analysis of co-occurrences information by evaluating a new model that jointly takes environmental variables and co-occurrences as inputs of an end-to-end network. Results show that the environmental models are the best performing methods and that there is a significant amount of complementary information between co-occurrences and environment. Indeed, the model learned on both inputs allows a significant performance gain compared to the environmental model alone.},
  isbn = {978-3-030-28577-7},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\ZFGV6TQT\Deneu et al. - 2019 - Evaluation of Deep Species Distribution Models Usi.pdf}
}

@article{deneuVeryHighResolution2022,
  title = {Very {{High Resolution Species Distribution Modeling Based}} on {{Remote Sensing Imagery}}: {{How}} to {{Capture Fine-Grained}} and {{Large-Scale Vegetation Ecology With Convolutional Neural Networks}}?},
  shorttitle = {Very {{High Resolution Species Distribution Modeling Based}} on {{Remote Sensing Imagery}}},
  author = {Deneu, Benjamin and Joly, Alexis and Bonnet, Pierre and Servajean, Maximilien and Munoz, Fran{\c c}ois},
  year = {2022},
  month = may,
  journal = {Frontiers in Plant Science},
  volume = {13},
  pages = {839279},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.839279},
  urldate = {2023-04-21},
  abstract = {Species Distribution Models (SDMs) are fundamental tools in ecology for predicting the geographic distribution of species based on environmental data. They are also very useful from an application point of view, whether for the implementation of conservation plans for threatened species or for monitoring invasive species. The generalizability and spatial accuracy of an SDM depend very strongly on the type of model used and the environmental data used as explanatory variables. In this article, we study a country-wide species distribution model based on very high resolution (VHR) (1 m) remote sensing images processed by a convolutional neural network. We demonstrate that this model can capture landscape and habitat information at very fine spatial scales while providing overall better predictive performance than conventional models. Moreover, to demonstrate the ecological significance of the model, we propose an original analysis based on the t-distributed Stochastic Neighbor Embedding (t-SNE) dimension reduction technique. It allows visualizing the relation between input data and species traits or environment learned by the model as well as conducting some statistical tests verifying them. We also analyze the spatial mapping of the t-SNE dimensions at both national and local levels, showing the model benefit of automatically learning environmental variation at multiple scales.},
  pmcid = {PMC9122285},
  pmid = {35599901},
  file = {C:\Users\armin\Zotero\storage\X9Y9LPZY\Deneu et al. - 2022 - Very High Resolution Species Distribution Modeling.pdf}
}

@inproceedings{deng2009imagenet,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  publisher = {Ieee}
}

@article{dormannCollinearityReviewMethods2013,
  title = {Collinearity: A Review of Methods to Deal with It and a Simulation Study Evaluating Their Performance},
  shorttitle = {Collinearity},
  author = {Dormann, Carsten F. and Elith, Jane and Bacher, Sven and Buchmann, Carsten and Carl, Gudrun and Carr{\'e}, Gabriel and Marqu{\'e}z, Jaime R. Garc{\'i}a and Gruber, Bernd and Lafourcade, Bruno and Leit{\~a}o, Pedro J. and M{\"u}nkem{\"u}ller, Tamara and McClean, Colin and Osborne, Patrick E. and Reineking, Bj{\"o}rn and Schr{\"o}der, Boris and Skidmore, Andrew K. and Zurell, Damaris and Lautenbach, Sven},
  year = {2013},
  journal = {Ecography},
  volume = {36},
  number = {1},
  pages = {27--46},
  issn = {1600-0587},
  doi = {10.1111/j.1600-0587.2012.07348.x},
  urldate = {2023-04-20},
  abstract = {Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the `folk lore'-thresholds of correlation coefficients between predictor variables of {\textbar}r{\textbar} {$>$}0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.},
  langid = {english},
  file = {C\:\\Users\\armin\\Zotero\\storage\\Z4JAETXI\\Dormann et al. - 2013 - Collinearity a review of methods to deal with it .pdf;C\:\\Users\\armin\\Zotero\\storage\\F8FDVY9A\\j.1600-0587.2012.07348.html}
}

@article{estopinanDeepSpeciesDistribution2022,
  title = {Deep {{Species Distribution Modeling From Sentinel-2 Image Time-Series}}: {{A Global Scale Analysis}} on the {{Orchid Family}}},
  shorttitle = {Deep {{Species Distribution Modeling From Sentinel-2 Image Time-Series}}},
  author = {Estopinan, Joaquim and Servajean, Maximilien and Bonnet, Pierre and Munoz, Fran{\c c}ois and Joly, Alexis},
  year = {2022},
  journal = {Frontiers in Plant Science},
  volume = {13},
  issn = {1664-462X},
  urldate = {2023-04-20},
  abstract = {Species distribution models (SDMs) are widely used numerical tools that rely on correlations between geolocated presences (and possibly absences) and environmental predictors to model the ecological preferences of species. Recently, SDMs exploiting deep learning and remote sensing images have emerged and have demonstrated high predictive performance. In particular, it has been shown that one of the key advantages of these models (called deep-SDMs) is their ability to capture the spatial structure of the landscape, unlike prior models. In this paper, we examine whether the temporal dimension of remote sensing images can also be exploited by deep-SDMs. Indeed, satellites such as Sentinel-2 are now providing data with a high temporal revisit, and it is likely that the resulting time-series of images contain relevant information about the seasonal variations of the environment and vegetation. To confirm this hypothesis, we built a substantial and original dataset (called DeepOrchidSeries) aimed at modeling the distribution of orchids on a global scale based on Sentinel-2 image time series. It includes around 1 million occurrences of orchids worldwide, each being paired with a 12-month-long time series of high-resolution images (640 x 640 m RGB+IR patches centered on the geolocated observations). This ambitious dataset enabled us to train several deep-SDMs based on convolutional neural networks (CNNs) whose input was extended to include the temporal dimension. To quantify the contribution of the temporal dimension, we designed a novel interpretability methodology based on temporal permutation tests, temporal sampling, and temporal averaging. We show that the predictive performance of the model is greatly increased by the seasonality information contained in the temporal series. In particular, occurrence-poor species and diversity-rich regions are the ones that benefit the most from this improvement, revealing the importance of habitat's temporal dynamics to characterize species distribution.},
  file = {C:\Users\armin\Zotero\storage\ZQCN3JWD\Estopinan et al. - 2022 - Deep Species Distribution Modeling From Sentinel-2.pdf}
}

@misc{falbelLuzHigherLevel2023,
  title = {Luz: {{Higher Level}} '{{API}}' for 'Torch'},
  shorttitle = {Luz},
  author = {Falbel, Daniel and RStudio},
  year = {2023},
  month = apr,
  urldate = {2024-12-16},
  abstract = {A high level interface for 'torch' providing utilities to reduce the the amount of code needed for common tasks, abstract away torch details and make the same code work on both the 'CPU' and 'GPU'. It's flexible enough to support expressing a large range of models. It's heavily inspired by 'fastai' by Howard et al. (2020) {$<$}doi:10.48550/arXiv.2002.04688{$>$}, 'Keras' by Chollet et al. (2015) and 'PyTorch Lightning' by Falcon et al. (2019) {$<$}doi:10.5281/zenodo.3828935{$>$}.},
  copyright = {MIT + file LICENSE}
}

@manual{falbelLuzHigherLevel2024,
  type = {Manual},
  title = {Luz: {{Higher}} Level '{{API}}' for 'Torch'},
  author = {Falbel, Daniel},
  year = {2024}
}

@manual{falbelTorchTensorsNeural2024,
  type = {Manual},
  title = {Torch: {{Tensors}} and Neural Networks with '{{GPU}}' Acceleration},
  author = {Falbel, Daniel and Luraschi, Javier},
  year = {2024}
}

@manual{falbelTorchvisionModelsDatasets2024,
  type = {Manual},
  title = {Torchvision: {{Models}}, Datasets and Transformations for Images},
  author = {Falbel, Daniel},
  year = {2024}
}

@inproceedings{fawazTransferLearningTime2018,
  title = {Transfer Learning for Time Series Classification},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2018},
  month = dec,
  eprint = {1811.01533},
  primaryclass = {cs},
  pages = {1367--1376},
  doi = {10.1109/BigData.2018.8621990},
  urldate = {2024-12-23},
  abstract = {Transfer learning for deep neural networks is the process of first training a base network on a source dataset, and then transferring the learned features (the network's weights) to a second network to be trained on a target dataset. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike for image recognition problems, transfer learning techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved if the model is fine-tuned from a pre-trained neural network instead of training it from scratch. In this paper, we fill this gap by investigating how to transfer deep CNNs for the TSC task. To evaluate the potential of transfer learning, we performed extensive experiments using the UCR archive which is the largest publicly available TSC benchmark containing 85 datasets. For each dataset in the archive, we pre-trained a model and then fine-tuned it on the other datasets resulting in 7140 different deep neural networks. These experiments revealed that transfer learning can improve or degrade the model's predictions depending on the dataset used for transfer. Therefore, in an effort to predict the best source dataset for a given target dataset, we propose a new method relying on Dynamic Time Warping to measure inter-datasets similarities. We describe how our method can guide the transfer to choose the best source dataset leading to an improvement in accuracy on 71 out of 85 datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\V7MFS37D\\Fawaz et al. - 2018 - Transfer learning for time series classification.pdf;C\:\\Users\\armin\\Zotero\\storage\\F8G8ZI7A\\1811.html}
}

@manual{fischerMlr3torchDeepLearning2024,
  type = {Manual},
  title = {Mlr3torch: {{Deep}} Learning with 'Mlr3'},
  author = {Fischer, Sebastian and Binder, Martin},
  year = {2024}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-12-23},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\583V5JSA\Gal und Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@article{gomezvillaAutomaticWildAnimal2017,
  title = {Towards Automatic Wild Animal Monitoring: {{Identification}} of Animal Species in Camera-Trap Images Using Very Deep Convolutional Neural Networks},
  shorttitle = {Towards Automatic Wild Animal Monitoring},
  author = {Gomez Villa, Alexander and Salazar, Augusto and Vargas, Francisco},
  year = {2017},
  month = sep,
  journal = {Ecological Informatics},
  volume = {41},
  pages = {24--32},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2017.07.004},
  urldate = {2024-12-18},
  abstract = {Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4\% Top-1 and 60.4\% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9\% Top-1 and 98.1\% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed.},
  keywords = {Animal species recognition,Camera-trap,Deep convolutional neural networks,Snapshot Serengeti},
  file = {C\:\\Users\\armin\\Zotero\\storage\\V9CMWVJK\\Gomez Villa et al. - 2017 - Towards automatic wild animal monitoring Identifi.pdf;C\:\\Users\\armin\\Zotero\\storage\\UISJDD94\\S1574954116302047.html}
}

@article{guillera-arroitaMySpeciesDistribution2015,
  title = {Is My Species Distribution Model Fit for Purpose? {{Matching}} Data and Models to Applications},
  shorttitle = {Is My Species Distribution Model Fit for Purpose?},
  author = {{Guillera-Arroita}, Gurutzeta and {Lahoz-Monfort}, Jos{\'e} J. and Elith, Jane and Gordon, Ascelin and Kujala, Heini and Lentini, Pia E. and McCarthy, Michael A. and Tingley, Reid and Wintle, Brendan A.},
  year = {2015},
  journal = {Global Ecology and Biogeography},
  volume = {24},
  number = {3},
  pages = {276--292},
  issn = {1466-8238},
  doi = {10.1111/geb.12268},
  urldate = {2023-04-20},
  abstract = {Species distribution models (SDMs) are used to inform a range of ecological, biogeographical and conservation applications. However, users often underestimate the strong links between data type, model output and suitability for end-use. We synthesize current knowledge and provide a simple framework that summarizes how interactions between data type and the sampling process (i.e. imperfect detection and sampling bias) determine the quantity that is estimated by a SDM. We then draw upon the published literature and simulations to illustrate and evaluate the information needs of the most common ecological, biogeographical and conservation applications of SDM outputs. We find that, while predictions of models fitted to the most commonly available observational data (presence records) suffice for some applications, others require estimates of occurrence probabilities, which are unattainable without reliable absence records. Our literature review and simulations reveal that, while converting continuous SDM outputs into categories of assumed presence or absence is common practice, it is seldom clearly justified by the application's objective and it usually degrades inference. Matching SDMs to the needs of particular applications is critical to avoid poor scientific inference and management outcomes. This paper aims to help modellers and users assess whether their intended SDM outputs are indeed fit for purpose.},
  langid = {english},
  keywords = {Ecological niche model,habitat model,imperfect detection,presence-absence,presence-background,presence-only,prevalence,sampling bias},
  file = {C\:\\Users\\armin\\Zotero\\storage\\HDEBCFB3\\Guillera-Arroita et al. - 2015 - Is my species distribution model fit for purpose .pdf;C\:\\Users\\armin\\Zotero\\storage\\V7XYLIDL\\geb.html}
}

@inproceedings{haraCanSpatiotemporal3D2018,
  title = {Can {{Spatiotemporal 3D CNNs Retrace}} the {{History}} of {{2D CNNs}} and {{ImageNet}}?},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  year = {2018},
  pages = {6546--6555},
  urldate = {2024-12-19},
  file = {C:\Users\armin\Zotero\storage\TR8BYD79\Hara et al. - 2018 - Can Spatiotemporal 3D CNNs Retrace the History of .pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2024-12-25},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\armin\\Zotero\\storage\\TQ8J26R7\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\armin\\Zotero\\storage\\Y8K2FHAP\\1512.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  urldate = {2023-05-12},
  file = {C:\Users\armin\Zotero\storage\Y5KILQ5R\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heMaskRCNN2017,
  title = {Mask {{R-CNN}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = {2017},
  pages = {2961--2969},
  urldate = {2024-12-16},
  file = {C:\Users\armin\Zotero\storage\CWN5GJCA\He et al. - 2017 - Mask R-CNN.pdf}
}

@article{hu2024introduction,
  title = {Introduction to Deep Learning Methods for Multi-Species Predictions},
  author = {Hu, Yuqing and {Si-Moussi}, Sara and Thuiller, Wilfried},
  year = {2024},
  journal = {Methods in Ecology and Evolution},
  publisher = {Wiley Online Library}
}

@article{huIntroductionDeepLearning,
  title = {Introduction to Deep Learning Methods for Multi-Species Predictions},
  author = {Hu, Yuqing and {Si-Moussi}, Sara and Thuiller, Wilfried},
  journal = {Methods in Ecology and Evolution},
  volume = {n/a},
  number = {n/a},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.14466},
  urldate = {2024-12-18},
  abstract = {Predicting species distributions and entire communities is crucial for ecologists, to enhance our understanding of the drivers behind species distributions and community assembly and to provide quantitative data for conservation efforts. Popular species distribution models use statistical and machine learning methods but face limitations with multi-species predictions at the community level, hindered by scalability and data imbalance sensitivity. This paper explores the potential of deep learning methods to overcome these challenges and provide more accurate multi-species predictions. Specifically, we introduced four distinct deep learning models that use site {\texttimes} species community data but differ in their internal structure or on the input environmental data structure: (1) a multi-layer perceptron (MLP) model for tabular data (e.g. in-situ/raster climate or soil data), (2) a convolutional neural network (CNN) and (3) a vision transformer (ViT) models tailored for image data (e.g. aerial ortho-photographs, satellite imagery), and a multimodal model that integrates both tabular and image data. We also show how adapted loss functions can address imbalance issues. We applied these deep learning models to a plant community dataset comprising 130,582 vegetation surveys encompassing 2522 species located in the French Alps. The tabular environmental data consisted of climate, terrain and soil information, while the images were derived from aerial photographs. All models achieved approximately 70\% true skill statistics on hold-out data, demonstrating high predictive capacity for community data, the multimodal model being the best performing one. Additionally, we showcased how interpretability tools can illuminate community structure as seen by deep learning models. Deep learning models offer a broad array of features for predicting entire species communities. They handle imbalance issues and accommodate various data types, from tabular datasets to images, while also being equipped with insightful interpretation tools. The versatility extends to tabular datasets and images, with no clear superiority between the two. The last hidden layers can provide valuable features for modelling other species, and the trained models can be used to support transfer learning to related tasks. The field of ecology now possesses an additional, potent tool in its arsenal that can foster basic and fundamental research.},
  langid = {english},
  keywords = {co-occurrence,deep neural networks,explainable AI,species community,species distribution models},
  file = {C:\Users\armin\Zotero\storage\9P9TXTI2\2041-210X.html}
}

@article{ibrahimExplainableConvolutionalNeural2023,
  title = {Explainable {{Convolutional Neural Networks}}: {{A Taxonomy}}, {{Review}}, and {{Future Directions}}},
  shorttitle = {Explainable {{Convolutional Neural Networks}}},
  author = {Ibrahim, Rami and Shafiq, M. Omair},
  year = {2023},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {10},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3563691},
  urldate = {2023-05-31},
  abstract = {Convolutional neural networks (CNNs)               have shown promising results and have outperformed classical machine learning techniques in tasks such as image classification and object recognition. Their human-brain like structure enabled them to learn sophisticated features while passing images through their layers. However, their lack of explainability led to the demand for interpretations to justify their predictions. Research on               Explainable AI               or               XAI               has gained momentum to provide knowledge and insights into neural networks. This study summarizes the literature to gain more understanding of explainability in CNNs (i.e., Explainable Convolutional Neural Networks). We classify models that made efforts to improve the CNNs interpretation. We present and discuss taxonomies for XAI models that modify CNN architecture, simplify CNN representations, analyze feature relevance, and visualize interpretations. We review various metrics used to evaluate XAI interpretations. In addition, we discuss the applications and tasks of XAI models. This focused and extensive survey develops a perspective on this area by addressing suggestions for overcoming XAI interpretation challenges, like models' generalization, unifying evaluation criteria, building robust models, and providing interpretations with semantic descriptions. Our taxonomy can be a reference to motivate future research in interpreting neural networks.},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\DYM7ZH3J\Ibrahim und Shafiq - 2023 - Explainable Convolutional Neural Networks A Taxon.pdf}
}

@article{jordanMachineLearningTrends2015,
  title = {Machine Learning: {{Trends}}, Perspectives, and Prospects},
  shorttitle = {Machine Learning},
  author = {Jordan, M. I. and Mitchell, T. M.},
  year = {2015},
  month = jul,
  journal = {Science},
  volume = {349},
  number = {6245},
  pages = {255--260},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaa8415},
  urldate = {2024-12-16},
  abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
  file = {C:\Users\armin\Zotero\storage\MZGZ5SFQ\Jordan und Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2024-12-16},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  file = {C:\Users\armin\Zotero\storage\A8E7A8L4\Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf}
}

@misc{kalinowskiKerasInterfaceKeras2024,
  title = {Keras: {{R Interface}} to '{{Keras}}'},
  shorttitle = {Keras},
  author = {Kalinowski, Tomasz and Falbel, Daniel and Allaire, J. J. and Chollet, Fran{\c c}ois and RStudio and Google and Tang [ctb, Yuan and {cph} and Bijl, Wouter Van Der and Studer, Martin and Keydana, Sigrid},
  year = {2024},
  month = apr,
  urldate = {2024-12-16},
  abstract = {Interface to 'Keras' {$<$}https://keras.io{$>$}, a high-level neural networks 'API'. 'Keras' was developed with a focus on enabling fast experimentation, supports both convolution based networks and recurrent networks (as well as combinations of the two), and runs seamlessly on both 'CPU' and 'GPU' devices.},
  copyright = {MIT + file LICENSE},
  keywords = {HighPerformanceComputing,ModelDeployment}
}

@article{kattenbornConvolutionalNeuralNetworks2019,
  title = {Convolutional {{Neural Networks}} Enable Efficient, Accurate and Fine-Grained Segmentation of Plant Species and Communities from High-Resolution {{UAV}} Imagery},
  author = {Kattenborn, Teja and Eichel, Jana and Fassnacht, Fabian Ewald},
  year = {2019},
  month = nov,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {17656},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-53797-9},
  urldate = {2024-12-18},
  abstract = {Recent technological advances in remote sensing sensors and platforms, such as high-resolution satellite imagers or unmanned aerial vehicles (UAV), facilitate the availability of fine-grained earth observation data. Such data reveal vegetation canopies in high spatial detail. Efficient methods are needed to fully harness this unpreceded source of information for vegetation mapping. Deep learning algorithms such as Convolutional Neural Networks (CNN) are currently paving new avenues in the field of image analysis and computer vision. Using multiple datasets, we test a CNN-based segmentation approach (U-net) in combination with training data directly derived from visual interpretation of UAV-based high-resolution RGB imagery for fine-grained mapping of vegetation species and communities. We demonstrate that this approach indeed accurately segments and maps vegetation species and communities (at least 84\% accuracy). The fact that we only used RGB imagery suggests that plant identification at very high spatial resolutions is facilitated through spatial patterns rather than spectral information. Accordingly, the presented approach is compatible with low-cost UAV systems that are easy to operate and thus applicable to a wide range of users.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Biodiversity,Community ecology,Forestry,Invasive species},
  file = {C:\Users\armin\Zotero\storage\5XCQLN8N\Kattenborn et al. - 2019 - Convolutional Neural Networks enable efficient, ac.pdf}
}

@manual{keydanaTorchaudioInterfacePytorchs2023,
  type = {Manual},
  title = {Torchaudio: {{R Interface}} to 'pytorch''s 'Torchaudio'},
  author = {Keydana, Sigrid and Damiani, Athos and Falbel, Daniel},
  year = {2023}
}

@article{koenenInterpretingDeepNeural2024,
  title = {Interpreting Deep Neural Networks with the Package {{innsight}}},
  author = {Koenen, Niklas and Wright, Marvin N.},
  year = {2024},
  journal = {Journal of Statistical Software},
  volume = {111},
  number = {8},
  pages = {1--52},
  doi = {10.18637/jss.v111.i08}
}

@inproceedings{kopukluResourceEfficient3D2019,
  title = {Resource {{Efficient 3D Convolutional Neural Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}}},
  author = {Kopuklu, Okan and Kose, Neslihan and Gunduz, Ahmet and Rigoll, Gerhard},
  year = {2019},
  pages = {0--0},
  urldate = {2024-12-19},
  file = {C:\Users\armin\Zotero\storage\Z5HYJPIB\Kopuklu et al. - 2019 - Resource Efficient 3D Convolutional Neural Network.pdf}
}

@misc{kopukluResourceEfficient3D2021,
  title = {Resource {{Efficient 3D Convolutional Neural Networks}}},
  author = {K{\"o}p{\"u}kl{\"u}, Okan and Kose, Neslihan and Gunduz, Ahmet and Rigoll, Gerhard},
  year = {2021},
  month = oct,
  number = {arXiv:1904.02422},
  eprint = {1904.02422},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.02422},
  urldate = {2024-12-19},
  abstract = {Recently, convolutional neural networks with 3D kernels (3D CNNs) have been very popular in computer vision community as a result of their superior ability of extracting spatio-temporal features within video frames compared to 2D CNNs. Although there has been great advances recently to build resource efficient 2D CNN architectures considering memory and power budget, there is hardly any similar resource efficient architectures for 3D CNNs. In this paper, we have converted various well-known resource efficient 2D CNNs to 3D CNNs and evaluated their performance on three major benchmarks in terms of classification accuracy for different complexity levels. We have experimented on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to inspect the applicability of transfer learning. We have evaluated the run-time performance of each model on a single Titan XP GPU and a Jetson TX2 embedded system. The results of this study show that these models can be utilized for different types of real-world applications since they provide real-time performance with considerable accuracies and memory usage. Our analysis on different complexity levels shows that the resource efficient 3D CNNs should not be designed too shallow or narrow in order to save complexity. The codes and pretrained models used in this work are publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\armin\\Zotero\\storage\\3YGRDS7Y\\Köpüklü et al. - 2021 - Resource Efficient 3D Convolutional Neural Network.pdf;C\:\\Users\\armin\\Zotero\\storage\\VHI7S6SN\\1904.html}
}

@misc{kopukluResourceEfficient3D2021a,
  title = {Resource {{Efficient 3D Convolutional Neural Networks}}},
  author = {K{\"o}p{\"u}kl{\"u}, Okan and Kose, Neslihan and Gunduz, Ahmet and Rigoll, Gerhard},
  year = {2021},
  month = oct,
  number = {arXiv:1904.02422},
  eprint = {1904.02422},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.02422},
  urldate = {2024-12-23},
  abstract = {Recently, convolutional neural networks with 3D kernels (3D CNNs) have been very popular in computer vision community as a result of their superior ability of extracting spatio-temporal features within video frames compared to 2D CNNs. Although there has been great advances recently to build resource efficient 2D CNN architectures considering memory and power budget, there is hardly any similar resource efficient architectures for 3D CNNs. In this paper, we have converted various well-known resource efficient 2D CNNs to 3D CNNs and evaluated their performance on three major benchmarks in terms of classification accuracy for different complexity levels. We have experimented on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to inspect the applicability of transfer learning. We have evaluated the run-time performance of each model on a single Titan XP GPU and a Jetson TX2 embedded system. The results of this study show that these models can be utilized for different types of real-world applications since they provide real-time performance with considerable accuracies and memory usage. Our analysis on different complexity levels shows that the resource efficient 3D CNNs should not be designed too shallow or narrow in order to save complexity. The codes and pretrained models used in this work are publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\armin\Zotero\storage\VUELEA66\Köpüklü et al. - 2021 - Resource Efficient 3D Convolutional Neural Network.pdf}
}

@article{lapuschkinUnmaskingCleverHans2019,
  title = {Unmasking {{Clever Hans}} Predictors and Assessing What Machines Really Learn},
  author = {Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2019},
  month = mar,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1096},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-08987-4},
  urldate = {2023-06-14},
  abstract = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Computer science,Machine learning},
  file = {C:\Users\armin\Zotero\storage\5IITK3RS\Lapuschkin et al. - 2019 - Unmasking Clever Hans predictors and assessing wha.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2024-12-17},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  file = {C:\Users\armin\Zotero\storage\69Z8YBFV\6795724.html}
}

@article{lecunBackpropagationAppliedHandwritten1989a,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2023-04-21},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\V8XHUANZ\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  urldate = {2024-12-17},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C:\Users\armin\Zotero\storage\DYPQLQTN\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf}
}

@article{liDeepLearningRemote2018,
  title = {Deep Learning for Remote Sensing Image Classification: {{A}} Survey},
  shorttitle = {Deep Learning for Remote Sensing Image Classification},
  author = {Li, Ying and Zhang, Haokui and Xue, Xizhe and Jiang, Yenan and Shen, Qiang},
  year = {2018},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {8},
  number = {6},
  pages = {e1264},
  issn = {1942-4795},
  doi = {10.1002/widm.1264},
  urldate = {2024-12-18},
  abstract = {Remote sensing (RS) image classification plays an important role in the earth observation technology using RS data, having been widely exploited in both military and civil fields. However, due to the characteristics of RS data such as high dimensionality and relatively small amounts of labeled samples available, performing RS image classification faces great scientific and practical challenges. In recent years, as new deep learning (DL) techniques emerge, approaches to RS image classification with DL have achieved significant breakthroughs, offering novel opportunities for the research and development of RS image classification. In this paper, a brief overview of typical DL models is presented first. This is followed by a systematic review of pixel-wise and scene-wise RS image classification approaches that are based on the use of DL. A comparative analysis regarding the performances of typical DL-based RS methods is also provided. Finally, the challenges and potential directions for further research are discussed. This article is categorized under: Application Areas {$>$} Science and Technology Technologies {$>$} Classification},
  copyright = {{\copyright} 2018 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {convolutional neural network,deep belief network,deep learning,pixel-wise classification,remote sensing image,scene classification,stacked auto-encoder},
  file = {C\:\\Users\\armin\\Zotero\\storage\\VV8D8NVY\\Li et al. - 2018 - Deep learning for remote sensing image classificat.pdf;C\:\\Users\\armin\\Zotero\\storage\\2BGIBJAX\\widm.html}
}

@article{liSurveyConvolutionalNeural2022,
  title = {A {{Survey}} of {{Convolutional Neural Networks}}: {{Analysis}}, {{Applications}}, and {{Prospects}}},
  shorttitle = {A {{Survey}} of {{Convolutional Neural Networks}}},
  author = {Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {12},
  pages = {6999--7019},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3084827},
  urldate = {2024-12-17},
  abstract = {A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.},
  keywords = {Computer vision,Convolutional neural networks,convolutional neural networks (CNNs),deep learning,Deep learning,deep neural networks,Feature extraction,Neurons},
  file = {C\:\\Users\\armin\\Zotero\\storage\\F76ADYKG\\Li et al. - 2022 - A Survey of Convolutional Neural Networks Analysi.pdf;C\:\\Users\\armin\\Zotero\\storage\\BMRGDG6N\\9451544.html}
}

@article{litjensSurveyDeepLearning2017,
  title = {A Survey on Deep Learning in Medical Image Analysis},
  author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and {van der Laak}, Jeroen A. W. M. and {van Ginneken}, Bram and S{\'a}nchez, Clara I.},
  year = {2017},
  month = dec,
  journal = {Medical Image Analysis},
  volume = {42},
  pages = {60--88},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.07.005},
  urldate = {2024-12-27},
  abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
  keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
  file = {C\:\\Users\\armin\\Zotero\\storage\\TIB845CW\\Litjens et al. - 2017 - A survey on deep learning in medical image analysi.pdf;C\:\\Users\\armin\\Zotero\\storage\\FBKLYVTK\\S1361841517301135.html}
}

@inproceedings{liuInterpretableDeepConvolutional2018,
  title = {Interpretable {{Deep Convolutional Neural Networks}} via {{Meta-learning}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
  year = {2018},
  month = jul,
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2018.8489172},
  abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for ``algorithmic fairness'' also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
  keywords = {big data,Computational modeling,Convolutional Neural Network,deep learning,interpretability,Machine learning,Machine learning algorithms,Meta-learning,Prediction algorithms,Predictive models,TensorFlow,Training data,Visualization},
  file = {C\:\\Users\\armin\\Zotero\\storage\\L7XICYQF\\Liu et al. - 2018 - Interpretable Deep Convolutional Neural Networks v.pdf;C\:\\Users\\armin\\Zotero\\storage\\6A8MJ87T\\metrics.html}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer Using Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  pages = {10012--10022},
  urldate = {2023-05-12},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\A5AF7V8Q\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@article{macaodhaBatDetectiveDeep2018,
  title = {Bat Detective---{{Deep}} Learning Tools for Bat Acoustic Signal Detection},
  author = {Mac Aodha, Oisin and Gibb, Rory and Barlow, Kate E. and Browning, Ella and Firman, Michael and Freeman, Robin and Harder, Briana and Kinsey, Libby and Mead, Gary R. and Newson, Stuart E. and Pandourski, Ivan and Parsons, Stuart and Russ, Jon and {Szodoray-Paradi}, Abigel and {Szodoray-Paradi}, Farkas and Tilova, Elena and Girolami, Mark and Brostow, Gabriel and Jones, Kate E.},
  editor = {Fenton, Brock},
  year = {2018},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {3},
  pages = {e1005995},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005995},
  urldate = {2024-12-27},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\2NDM8933\Mac Aodha et al. - 2018 - Bat detective—Deep learning tools for bat acoustic.pdf}
}

@article{martinsDeepLearningbasedTree2021,
  title = {Deep Learning-Based Tree Species Mapping in a Highly Diverse Tropical Urban Setting},
  author = {Martins, Gabriela Barbosa and La Rosa, Laura Elena Cu{\'e} and Happ, Patrick Nigri and Filho, Luiz Carlos Teixeira Coelho and Santos, Celso Junius F. and Feitosa, Raul Queiroz and Ferreira, Matheus Pinheiro},
  year = {2021},
  month = sep,
  journal = {Urban Forestry \& Urban Greening},
  volume = {64},
  pages = {127241},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2021.127241},
  urldate = {2024-12-18},
  abstract = {Spatially explicit information on urban tree species distribution is crucial for green infrastructure management in cities. This information is usually acquired with ground-based surveys, which are time-consuming and usually cover limited spatial extents. The combination of machine learning algorithms and remote sensing images has been hailed as a promising way to map tree species over broad areas. Recently, convolutional neural networks (CNNs), a type of deep learning method, have achieved outstanding results for tree species discrimination in various remote sensing data types. However, there is a lack of studies using CNN-based methods to produce tree species composition maps, particularly for tropical urban settings. Here, we propose a multi-task CNN to map tree species in a highly diverse neighborhood in Rio de Janeiro, Brazil. Our network architecture takes an aerial photograph (RGB bands and pixel size = 0.15 m) and delivers two outputs: a semantically segmented image and a distance map transform. In the former, all pixel positions are labeled, while in the latter, each pixel position contains the Euclidean distance to the crown boundary. We developed a post-processing approach that combines the two outputs, and we classified nine and five tree species with an average F1-score of 79.3\,{\textpm}\,8.6\% and 87.6\,{\textpm}\,4.4\%, respectively. Moreover, our post-processing approach produced a realistic tree species composition map by labeling only pixels of the target species with high class membership probabilities. Our results show the potential of CNNs and aerial photographs to map tree species in highly diverse tropical urban settings, providing valuable insights for urban forest management and green spaces planning.},
  keywords = {Convolutional neural networks,Deep learning,Remote sensing,Tree species discrimination},
  file = {C:\Users\armin\Zotero\storage\H7XVCWUF\S1618866721002661.html}
}

@article{mlr3,
  title = {{{mlr3}}: {{A}} Modern Object-Oriented Machine Learning Framework in {{R}}},
  author = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
  year = {2019},
  month = dec,
  journal = {Journal of Open Source Software},
  doi = {10.21105/joss.01903}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  urldate = {2023-04-20},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {C:\Users\armin\Zotero\storage\7HN4SKA9\interpretable-ml-book.html}
}

@book{molnarInterpretableMachineLearning2020,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  year = {2020},
  publisher = {Lulu.com},
  abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
  googlebooks = {jBm3DwAAQBAJ},
  isbn = {978-0-244-76852-2},
  langid = {english}
}

@article{moradi2020survey,
  title = {A Survey of Regularization Strategies for Deep Models},
  author = {Moradi, Reza and Berangi, Reza and Minaei, Behrouz},
  year = {2020},
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {6},
  pages = {3947--3986},
  publisher = {Springer}
}

@article{norouzzadehAutomaticallyIdentifyingCounting2018,
  title = {Automatically Identifying, Counting, and Describing Wild Animals in Camera-Trap Images with Deep Learning},
  author = {Norouzzadeh, Mohammad Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Alexandra and Palmer, Meredith S. and Packer, Craig and Clune, Jeff},
  year = {2018},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {25},
  pages = {E5716-E5725},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1719367115},
  urldate = {2024-12-18},
  abstract = {Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into ``big data'' sciences. Motion-sensor ``camera traps'' enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with {$>$}93.8\% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3\% of the data while still performing at the same 96.6\% accuracy as that of crowdsourced teams of human volunteers, saving {$>$}8.4 y (i.e., {$>$}17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
  file = {C:\Users\armin\Zotero\storage\A7DYJQLM\Norouzzadeh et al. - 2018 - Automatically identifying, counting, and describin.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-16},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  file = {C:\Users\armin\Zotero\storage\KF57A4WM\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@article{pichlerMachineLearningAlgorithms2020,
  title = {Machine Learning Algorithms to Infer Trait-Matching and Predict Species Interactions in Ecological Networks},
  author = {Pichler, Maximilian and Boreux, Virginie and Klein, Alexandra-Maria and Schleuning, Matthias and Hartig, Florian},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {2},
  pages = {281--293},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13329},
  urldate = {2024-12-18},
  abstract = {Ecologists have long suspected that species are more likely to interact if their traits match in a particular way. For example, a pollination interaction may be more likely if the proportions of a bee's tongue fit a plant's flower shape. Empirical estimates of the importance of trait-matching for determining species interactions, however, vary significantly among different types of ecological networks. Here, we show that ambiguity among empirical trait-matching studies may have arisen at least in parts from using overly simple statistical models. Using simulated and real data, we contrast conventional generalized linear models (GLM) with more flexible Machine Learning (ML) models (Random Forest, Boosted Regression Trees, Deep Neural Networks, Convolutional Neural Networks, Support Vector Machines, na{\"i}ve Bayes, and k-Nearest-Neighbor), testing their ability to predict species interactions based on traits, and infer trait combinations causally responsible for species interactions. We found that the best ML models can successfully predict species interactions in plant--pollinator networks, outperforming GLMs by a substantial margin. Our results also demonstrate that ML models can better identify the causally responsible trait-matching combinations than GLMs. In two case studies, the best ML models successfully predicted species interactions in a global plant--pollinator database and inferred ecologically plausible trait-matching rules for a plant--hummingbird network from Costa Rica, without any prior assumptions about the system. We conclude that flexible ML models offer many advantages over traditional regression models for understanding interaction networks. We anticipate that these results extrapolate to other ecological network types. More generally, our results highlight the potential of machine learning and artificial intelligence for inference in ecology, beyond standard tasks such as image or pattern recognition.},
  copyright = {{\copyright} 2019 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {bipartite networks,causal inference,deep learning,hummingbirds,insect pollinators,machine learning,pollination syndromes,predictive modelling},
  file = {C\:\\Users\\armin\\Zotero\\storage\\PN2RVR2H\\Pichler et al. - 2020 - Machine learning algorithms to infer trait-matchin.pdf;C\:\\Users\\armin\\Zotero\\storage\\XG287RMP\\2041-210X.html}
}

@article{pichlerMachineLearningDeep2023,
  title = {Machine Learning and Deep Learning---{{A}} Review for Ecologists},
  author = {Pichler, Maximilian and Hartig, Florian},
  year = {2023},
  journal = {Methods in Ecology and Evolution},
  volume = {14},
  number = {4},
  pages = {994--1016},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.14061},
  urldate = {2024-12-16},
  abstract = {The popularity of machine learning (ML), deep learning (DL) and artificial intelligence (AI) has risen sharply in recent years. Despite this spike in popularity, the inner workings of ML and DL algorithms are often perceived as opaque, and their relationship to classical data analysis tools remains debated. Although it is often assumed that ML and DL excel primarily at making predictions, ML and DL can also be used for analytical tasks traditionally addressed with statistical models. Moreover, most recent discussions and reviews on ML focus mainly on DL, failing to synthesise the wealth of ML algorithms with different advantages and general principles. Here, we provide a comprehensive overview of the field of ML and DL, starting by summarizing its historical developments, existing algorithm families, differences to traditional statistical tools, and universal ML principles. We then discuss why and when ML and DL models excel at prediction tasks and where they could offer alternatives to traditional statistical methods for inference, highlighting current and emerging applications for ecological problems. Finally, we summarize emerging trends such as scientific and causal ML, explainable AI, and responsible AI that may significantly impact ecological data analysis in the future. We conclude that ML and DL are powerful new tools for predictive modelling and data analysis. The superior performance of ML and DL algorithms compared to statistical models can be explained by their higher flexibility and automatic data-dependent complexity optimization. However, their use for causal inference is still disputed as the focus of ML and DL methods on predictions creates challenges for the interpretation of these models. Nevertheless, we expect ML and DL to become an indispensable tool in ecology and evolution, comparable to other traditional statistical tools.},
  copyright = {{\copyright} 2023 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {artificial intelligence,big data,causal inference,deep learning,machine learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\D4GJLJMD\\Pichler und Hartig - 2023 - Machine learning and deep learning—A review for ec.pdf;C\:\\Users\\armin\\Zotero\\storage\\MX5GLTAG\\2041-210X.html}
}

@incollection{prechelt2002early,
  title = {Early Stopping-but When?},
  booktitle = {Neural Networks: {{Tricks}} of the Trade},
  author = {Prechelt, Lutz},
  year = {2002},
  pages = {55--69},
  publisher = {Springer}
}

@article{qianUAVDeepConvolutional2020,
  title = {{{UAV}} and a Deep Convolutional Neural Network for Monitoring Invasive Alien Plants in the Wild},
  author = {Qian, Wanqiang and Huang, Yiqi and Liu, Qi and Fan, Wei and Sun, Zhongyu and Dong, Hui and Wan, Fanghao and Qiao, Xi},
  year = {2020},
  month = jul,
  journal = {Computers and Electronics in Agriculture},
  volume = {174},
  pages = {105519},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2020.105519},
  urldate = {2024-12-18},
  abstract = {Invasive alien plants (IAPs) are considered to be among the greatest global threats to biodiversity and ecosystems. Timely and effective monitoring is important for their prevention and control. However, monitoring remains mainly dependent on satellite remote sensing and manual inspection, which has a high cost and rather low accuracy and efficiency. We considered that this problem could be solved using unmanned aerial vehicle (UAV) intelligent monitoring. Accurate and rapid identification of IAPs in the wild is the core of intelligent monitoring. We intended to acquire colour images of the monitoring area in a field environment using an UAV and proposing a novel IAPsNet based on a deep convolutional neural network (CNN) to identify the IAPs appearing in the images. 6400 samples were one by one manually divided into seven IAP categories and one background category as training set. IAPsNet incorporated AlexNet local response normalization (LRN), GoogLeNet inception models, and continuous VGG convolution. Through training and testing, the IAPsNet performance for 893 testing samples was rather satisfactory, reaching an accuracy of 93.39\% within a time of 1.8846\,s and the average recall, average precision and average F1-score can reach 93.3\%, 93.74\% and 93.52\% respectively. Moreover, in quantitative and qualitative comparative analysis, IAPsNet not only has high accuracy, high recall, high precision, high F1-score and efficiency but also has a high anti-interference capacity against blur, environment and multi-scales. Additionally, IAPsNet was applied to 4 different real wild conditions, proving that it is able to adapt to different scenes and simultaneously identify multiple species; it has potential to be used in the wild. High-quality distributional data of invasive plants are provided for subsequent ecological analysis. The data will help management authorities to implement the necessary steps in an identified area to develop a comprehensive strategy for IAP control.},
  keywords = {Computer vision,Deep learning,Invasive alien plant,Multi-object identification},
  file = {C:\Users\armin\Zotero\storage\4HJ55RPZ\S0168169920302921.html}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2024-12-23},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  file = {C:\Users\armin\Zotero\storage\TX5P49S8\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{robertsCrossvalidationStrategiesData2017,
  title = {Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure},
  author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and {Guillera-Arroita}, Gurutzeta and Hauenstein, Severin and {Lahoz-Monfort}, Jos{\'e} J. and Schr{\"o}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
  year = {2017},
  journal = {Ecography},
  volume = {40},
  number = {8},
  pages = {913--929},
  issn = {1600-0587},
  doi = {10.1111/ecog.02881},
  urldate = {2023-04-20},
  abstract = {Ecological data often show temporal, spatial, hierarchical (random effects), or phylogenetic structure. Modern statistical approaches are increasingly accounting for such dependencies. However, when performing cross-validation, these structures are regularly ignored, resulting in serious underestimation of predictive error. One cause for the poor performance of uncorrected (random) cross-validation, noted often by modellers, are dependence structures in the data that persist as dependence structures in model residuals, violating the assumption of independence. Even more concerning, because often overlooked, is that structured data also provides ample opportunity for overfitting with non-causal predictors. This problem can persist even if remedies such as autoregressive models, generalized least squares, or mixed models are used. Block cross-validation, where data are split strategically rather than randomly, can address these issues. However, the blocking strategy must be carefully considered. Blocking in space, time, random effects or phylogenetic distance, while accounting for dependencies in the data, may also unwittingly induce extrapolations by restricting the ranges or combinations of predictor variables available for model training, thus overestimating interpolation errors. On the other hand, deliberate blocking in predictor space may also improve error estimates when extrapolation is the modelling goal. Here, we review the ecological literature on non-random and blocked cross-validation approaches. We also provide a series of simulations and case studies, in which we show that, for all instances tested, block cross-validation is nearly universally more appropriate than random cross-validation if the goal is predicting to new data or predictor space, or for selecting causal predictors. We recommend that block cross-validation be used wherever dependence structures exist in a dataset, even if no correlation structure is visible in the fitted model residuals, or if the fitted models account for such correlations.},
  langid = {english},
  file = {C\:\\Users\\armin\\Zotero\\storage\\C86YEZVZ\\Roberts et al. - 2017 - Cross-validation strategies for data with temporal.pdf;C\:\\Users\\armin\\Zotero\\storage\\9D6PB9LY\\ecog.html}
}

@article{roveroWhichCameraTrap2013,
  title = {"{{Which}} Camera Trap Type and How Many Do {{I}} Need?" {{A}} Review of Camera Features and Study Designs for a Range of Wildlife Research Applications},
  shorttitle = {"{{Which}} Camera Trap Type and How Many Do {{I}} Need?},
  author = {Rovero, Francesco and Zimmermann, Fridolin and Berzi, Duccio and Meek, Paul},
  year = {2013},
  month = aug,
  journal = {Hystrix, the Italian Journal of Mammalogy},
  volume = {24},
  number = {2},
  pages = {148--156},
  publisher = {Associazione Teriologica Italiana},
  issn = {0394-1914, 1825-5272},
  doi = {10.4404/hystrix-24.2-8789},
  urldate = {2024-12-18},
  abstract = {Automatically triggered cameras taking photographs or videos of passing animals (camera traps) have emerged over the last decade as one of the most powerful tool for wildlife research. In parallel, a wealth of camera trap systems and models has become commercially available, a phenomenon mainly...},
  langid = {english}
}

@article{ryoExplainableArtificialIntelligence2021,
  title = {Explainable Artificial Intelligence Enhances the Ecological Interpretability of Black-Box Species Distribution Models},
  author = {Ryo, Masahiro and Angelov, Boyan and Mammola, Stefano and Kass, Jamie M. and Benito, Blas M. and Hartig, Florian},
  year = {2021},
  journal = {Ecography},
  volume = {44},
  number = {2},
  pages = {199--205},
  issn = {1600-0587},
  doi = {10.1111/ecog.05360},
  urldate = {2023-04-20},
  abstract = {Species distribution models (SDMs) are widely used in ecology, biogeography and conservation biology to estimate relationships between environmental variables and species occurrence data and make predictions of how their distributions vary in space and time. During the past two decades, the field has increasingly made use of machine learning approaches for constructing and validating SDMs. Model accuracy has steadily increased as a result, but the interpretability of the fitted models, for example the relative importance of predictor variables or their causal effects on focal species, has not always kept pace. Here we draw attention to an emerging subdiscipline of artificial intelligence, explainable AI (xAI), as a toolbox for better interpreting SDMs. xAI aims at deciphering the behavior of complex statistical or machine learning models (e.g. neural networks, random forests, boosted regression trees), and can produce more transparent and understandable SDM predictions. We describe the rationale behind xAI and provide a list of tools that can be used to help ecological modelers better understand complex model behavior at different scales. As an example, we perform a reproducible SDM analysis in R on the African elephant and showcase some xAI tools such as local interpretable model-agnostic explanation (LIME) to help interpret local-scale behavior of the model. We conclude with what we see as the benefits and caveats of these techniques and advocate for their use to improve the interpretability of machine learning SDMs.},
  langid = {english},
  keywords = {ecological modeling,explainable artificial intelligence,habitat suitability modeling,interpretable machine learning,species distribution model,xAI},
  file = {C\:\\Users\\armin\\Zotero\\storage\\LCHDVH5B\\Ryo et al. - 2021 - Explainable artificial intelligence enhances the e.pdf;C\:\\Users\\armin\\Zotero\\storage\\XXEACZIM\\ecog.html}
}

@inproceedings{salamonFusingShallowDeep2017,
  title = {Fusing Shallow and Deep Learning for Bioacoustic Bird Species Classification},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Salamon, Justin and Bello, Juan Pablo and Farnsworth, Andrew and Kelling, Steve},
  year = {2017},
  month = mar,
  pages = {141--145},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7952134},
  urldate = {2024-12-18},
  abstract = {Automated classification of organisms to species based on their vocalizations would contribute tremendously to abilities to monitor biodiversity, with a wide range of applications in the field of ecology. In particular, automated classification of migrating birds' flight calls could yield new biological insights and conservation applications for birds that vocalize during migration. In this paper we explore state-of-the-art classification techniques for large-vocabulary bird species classification from flight calls. In particular, we contrast a ``shallow learning'' approach based on unsupervised dictionary learning with a deep convolutional neural network combined with data augmentation. We show that the two models perform comparably on a dataset of 5428 flight calls spanning 43 different species, with both significantly outperforming an MFCC baseline. Finally, we show that by combining the models using a simple late-fusion approach we can further improve the results, obtaining a state-of-the-art classification accuracy of 0.96.},
  keywords = {bioacoustics,Birds,Convolutional codes,Convolutional neural networks,data augmentation,deep learning,Dictionaries,flight calls,Machine learning,Mel frequency cepstral coefficient,Monitoring,Neural networks},
  file = {C\:\\Users\\armin\\Zotero\\storage\\TVIUMYVR\\Salamon et al. - 2017 - Fusing shallow and deep learning for bioacoustic b.pdf;C\:\\Users\\armin\\Zotero\\storage\\LEFKC826\\7952134.html}
}

@misc{sandlerMobileNetV2InvertedResiduals2019,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2019},
  month = mar,
  number = {arXiv:1801.04381},
  eprint = {1801.04381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.04381},
  urldate = {2024-12-19},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\armin\\Zotero\\storage\\DGJYM2R2\\Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;C\:\\Users\\armin\\Zotero\\storage\\MH2SQ7H8\\1801.html}
}

@inproceedings{sastryBirdSATCrossViewContrastive2024,
  title = {{{BirdSAT}}: {{Cross-View Contrastive Masked Autoencoders}} for {{Bird Species Classification}} and {{Mapping}}},
  shorttitle = {{{BirdSAT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Sastry, Srikumar and Khanal, Subash and Dhakal, Aayush and Huang, Di and Jacobs, Nathan},
  year = {2024},
  pages = {7136--7145},
  urldate = {2024-12-24},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\YZXIJEHE\Sastry et al. - 2024 - BirdSAT Cross-View Contrastive Masked Autoencoder.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization},
  file = {C\:\\Users\\armin\\Zotero\\storage\\5HPFY6ZL\\Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\armin\\Zotero\\storage\\XA9DYRPX\\8237336.html}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2024-12-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\T3DPU2F2\\Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\armin\\Zotero\\storage\\XS364AFT\\1610.html}
}

@article{selvarajuGradCAMVisualExplanations2020a,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2024-12-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\5SC3P8J8\\Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\armin\\Zotero\\storage\\TD6FZ98F\\1610.html}
}

@article{shafer2008tutorial,
  title = {A Tutorial on Conformal Prediction.},
  author = {Shafer, Glenn and Vovk, Vladimir},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {3}
}

@book{silgeTidyModeling,
  title = {Tidy {{Modeling}} with {{R}}},
  author = {Silge, Max Kuhn {and} Julia},
  urldate = {2023-04-20},
  abstract = {The tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles. This book provides a thorough introduction to how to use tidymodels, and an outline of good methodology and statistical practice for phases of the modeling process.},
  file = {C:\Users\armin\Zotero\storage\YU6DZUH9\www.tmwr.org.html}
}

@article{silver2016mastering,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  year = {2016},
  journal = {nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group}
}

@misc{silverMasteringChessShogi2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  number = {arXiv:1712.01815},
  eprint = {1712.01815},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.01815},
  urldate = {2024-12-16},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\MVM3KRDF\\Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;C\:\\Users\\armin\\Zotero\\storage\\E8QDPGCE\\1712.html}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2024-12-25},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\armin\\Zotero\\storage\\4LXWYDBU\\Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\armin\\Zotero\\storage\\RRUP6XMD\\1409.html}
}

@misc{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  month = jun,
  number = {arXiv:1706.03825},
  eprint = {1706.03825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03825},
  urldate = {2024-12-23},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\2D73F7NZ\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf;C\:\\Users\\armin\\Zotero\\storage\\LVUNAKQI\\1706.html}
}

@article{stevensScaleMeasurementPsychological1937,
  title = {A {{Scale}} for the {{Measurement}} of the {{Psychological Magnitude Pitch}}},
  author = {Stevens, S. S. and Volkmann, J. and Newman, E. B.},
  year = {1937},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {8},
  number = {3},
  pages = {185--190},
  issn = {0001-4966},
  doi = {10.1121/1.1915893},
  urldate = {2024-12-21},
  abstract = {A subjective scale for the measurement of pitch was constructed from determinations of the half-value of pitches at various frequencies. This scale differs from both the musical scale and the frequency scale, neither of which is subjective. Five observers fractionated tones of 10 different frequencies at a loudness level of 60 db. From these fractionations a numerical scale was constructed which is proportional to the perceived magnitude of subjective pitch. In numbering the scale the 1000-cycle tone was assigned the pitch of 1000 subjective units (mels). The close agreement of the pitch scale with an integration of the differential thresholds (DL's) shows that, unlike the DL's for loudness, all DL's for pitch are of uniform subjective magnitude. The agreement further implies that pitch and differential sensitivity to pitch are both rectilinear functions of extent on the basilar membrane. The correspondence of the pitch scale and the experimentally determined location of the resonant areas of the basilar membrane suggests that, in cutting a pitch in half, the observer adjusts the tone until it stimulates a position half-way from the original locus to the apical end of the membrane. Measurement of the subjective size of musical intervals (such as octaves) in terms of the pitch scale shows that the intervals become larger as the frequency of the mid-point of the interval increases (except in the two highest audible octaves). This result confirms earlier judgments as to the relative size of octaves in different parts of the frequency range.},
  file = {C:\Users\armin\Zotero\storage\EE3DV3S2\A-Scale-for-the-Measurement-of-the-Psychological.html}
}

@article{tabakMachineLearningClassify2019,
  title = {Machine Learning to Classify Animal Species in Camera Trap Images: {{Applications}} in Ecology},
  shorttitle = {Machine Learning to Classify Animal Species in Camera Trap Images},
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585--590},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13120},
  urldate = {2024-12-18},
  abstract = {Motion-activated cameras (``camera traps'') are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or ``out-of-distribution'' in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  copyright = {{\copyright} 2018 The Authors. Methods in Ecology and Evolution {\copyright} 2018 British Ecological Society},
  langid = {english},
  keywords = {artificial intelligence,camera trap,convolutional neural network,deep neural networks,image classification,machine learning,r package,remote sensing},
  file = {C\:\\Users\\armin\\Zotero\\storage\\GFTNP8XD\\Tabak et al. - 2019 - Machine learning to classify animal species in cam.pdf;C\:\\Users\\armin\\Zotero\\storage\\3M2F2T2G\\2041-210X.html}
}

@article{tuiaPerspectivesMachineLearning2022,
  title = {Perspectives in Machine Learning for Wildlife Conservation},
  author = {Tuia, Devis and Kellenberger, Benjamin and Beery, Sara and Costelloe, Blair R. and Zuffi, Silvia and Risse, Benjamin and Mathis, Alexander and Mathis, Mackenzie W. and {van Langevelde}, Frank and Burghardt, Tilo and Kays, Roland and Klinck, Holger and Wikelski, Martin and Couzin, Iain D. and {van Horn}, Grant and Crofoot, Margaret C. and Stewart, Charles V. and {Berger-Wolf}, Tanya},
  year = {2022},
  month = feb,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {792},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-27980-y},
  urldate = {2024-12-16},
  abstract = {Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Conservation biology},
  file = {C:\Users\armin\Zotero\storage\R5BP7G4D\Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf}
}

@article{valaviPredictivePerformancePresenceonly2022,
  title = {Predictive Performance of Presence-Only Species Distribution Models: A Benchmark Study with Reproducible Code},
  shorttitle = {Predictive Performance of Presence-Only Species Distribution Models},
  author = {Valavi, Roozbeh and {Guillera-Arroita}, Gurutzeta and {Lahoz-Monfort}, Jos{\'e} J. and Elith, Jane},
  year = {2022},
  journal = {Ecological Monographs},
  volume = {92},
  number = {1},
  pages = {e01486},
  issn = {1557-7015},
  doi = {10.1002/ecm.1486},
  urldate = {2023-04-28},
  abstract = {Species distribution modeling (SDM) is widely used in ecology and conservation. Currently, the most available data for SDM are species presence-only records (available through digital databases). There have been many studies comparing the performance of alternative algorithms for modeling presence-only data. Among these, a 2006 paper from Elith and colleagues has been particularly influential in the field, partly because they used several novel methods (at the time) on a global data set that included independent presence--absence records for model evaluation. Since its publication, some of the algorithms have been further developed and new ones have emerged. In this paper, we explore patterns in predictive performance across methods, by reanalyzing the same data set (225 species from six different regions) using updated modeling knowledge and practices. We apply well-established methods such as generalized additive models and MaxEnt, alongside others that have received attention more recently, including regularized regressions, point-process weighted regressions, random forests, XGBoost, support vector machines, and the ensemble modeling framework biomod. All the methods we use include background samples (a sample of environments in the landscape) for model fitting. We explore impacts of using weights on the presence and background points in model fitting. We introduce new ways of evaluating models fitted to these data, using the area under the precision-recall gain curve, and focusing on the rank of results. We find that the way models are fitted matters. The top method was an ensemble of tuned individual models. In contrast, ensembles built using the biomod framework with default parameters performed no better than single moderate performing models. Similarly, the second top performing method was a random forest parameterized to deal with many background samples (contrasted to relatively few presence records), which substantially outperformed other random forest implementations. We find that, in general, nonparametric techniques with the capability of controlling for model complexity outperformed traditional regression methods, with MaxEnt and boosted regression trees still among the top performing models. All the data and code with working examples are provided to make this study fully reproducible.},
  langid = {english},
  keywords = {boosted regression trees,down sampling,ecological niche model,ensemble modeling,imbalanced data,independent test data,machine learning,maxent,model evaluation,point process weighting,presence-background,random forest},
  file = {C\:\\Users\\armin\\Zotero\\storage\\IAQYVNFW\\Valavi et al. - 2022 - Predictive performance of presence-only species di.pdf;C\:\\Users\\armin\\Zotero\\storage\\FQ32GS55\\ecm.html}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {C:\Users\armin\Zotero\storage\RA7BNY4G\Vaswani et al. - Attention is All you Need.pdf}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2024-12-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\armin\\Zotero\\storage\\RV3N76ZI\\Vaswani et al. - 2017 - Attention Is All You Need.pdf;C\:\\Users\\armin\\Zotero\\storage\\SR454WWW\\1706.html}
}

@misc{viloneExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}}: A {{Systematic Review}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Vilone, Giulia and Longo, Luca},
  year = {2020},
  month = oct,
  number = {arXiv:2006.00093},
  eprint = {2006.00093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.00093},
  urldate = {2023-06-14},
  abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,I.2.m},
  file = {C\:\\Users\\armin\\Zotero\\storage\\9H3IAZ27\\Vilone und Longo - 2020 - Explainable Artificial Intelligence a Systematic .pdf;C\:\\Users\\armin\\Zotero\\storage\\75MTGV9B\\2006.html}
}

@article{vonluxburgTutorialSpectralClustering2007,
  title = {A Tutorial on Spectral Clustering},
  author = {{von Luxburg}, Ulrike},
  year = {2007},
  month = dec,
  journal = {Statistics and Computing},
  volume = {17},
  number = {4},
  pages = {395--416},
  issn = {1573-1375},
  doi = {10.1007/s11222-007-9033-z},
  urldate = {2023-06-15},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  langid = {english},
  keywords = {Graph Laplacian,Spectral clustering},
  file = {C:\Users\armin\Zotero\storage\WICZ4DTP\von Luxburg - 2007 - A tutorial on spectral clustering.pdf}
}

@article{wangDeepFaceRecognition2021,
  title = {Deep Face Recognition: {{A}} Survey},
  shorttitle = {Deep Face Recognition},
  author = {Wang, Mei and Deng, Weihong},
  year = {2021},
  month = mar,
  journal = {Neurocomputing},
  volume = {429},
  pages = {215--244},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.10.081},
  urldate = {2024-12-27},
  abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: ``one-to-many augmentation'' and ``many-to-one normalization''. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.},
  keywords = {Deep face recognition,Deep learning,Deep network architecture,Face processing,Face recognition database,Loss function},
  file = {C\:\\Users\\armin\\Zotero\\storage\\UATBV9EB\\Wang und Deng - 2021 - Deep face recognition A survey.pdf;C\:\\Users\\armin\\Zotero\\storage\\GMC3LKS9\\S0925231220316945.html}
}

@article{weimannTransferLearningECG2021,
  title = {Transfer Learning for {{ECG}} Classification},
  author = {Weimann, Kuba and Conrad, Tim O. F.},
  year = {2021},
  month = mar,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {5251},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-84374-8},
  urldate = {2024-12-19},
  abstract = {Remote monitoring devices, which can be worn or implanted, have enabled a more effective healthcare for patients with periodic heart arrhythmia due to their ability to constantly monitor heart activity. However, these devices record considerable amounts of electrocardiogram (ECG) data that needs to be interpreted by physicians. Therefore, there is a growing need to develop reliable methods for automatic ECG interpretation to assist the physicians. Here, we use deep convolutional neural networks (CNN) to classify raw ECG recordings. However, training CNNs for ECG classification often requires a large number of annotated samples, which are expensive to acquire. In this work, we tackle this problem by using transfer learning. First, we pretrain CNNs on the largest public data set of continuous raw ECG signals. Next, we finetune the networks on a small data set for classification of Atrial Fibrillation, which is the most common heart arrhythmia. We show that pretraining improves the performance of CNNs on the target task by up to \$\$6.57{\textbackslash}\%\$\$, effectively reducing the number of annotations required to achieve the same performance as CNNs that are not pretrained. We investigate both supervised as well as unsupervised pretraining approaches, which we believe will increase in relevance, since they do not rely on the expensive ECG annotations. The code is available on GitHub at https://github.com/kweimann/ecg-transfer-learning.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Machine learning},
  file = {C:\Users\armin\Zotero\storage\VWFQXI2G\Weimann und Conrad - 2021 - Transfer learning for ECG classification.pdf}
}

@article{weimannTransferLearningECG2021a,
  title = {Transfer Learning for {{ECG}} Classification},
  author = {Weimann, Kuba and Conrad, Tim O. F.},
  year = {2021},
  month = mar,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {5251},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-84374-8},
  urldate = {2024-12-23},
  abstract = {Remote monitoring devices, which can be worn or implanted, have enabled a more effective healthcare for patients with periodic heart arrhythmia due to their ability to constantly monitor heart activity. However, these devices record considerable amounts of electrocardiogram (ECG) data that needs to be interpreted by physicians. Therefore, there is a growing need to develop reliable methods for automatic ECG interpretation to assist the physicians. Here, we use deep convolutional neural networks (CNN) to classify raw ECG recordings. However, training CNNs for ECG classification often requires a large number of annotated samples, which are expensive to acquire. In this work, we tackle this problem by using transfer learning. First, we pretrain CNNs on the largest public data set of continuous raw ECG signals. Next, we finetune the networks on a small data set for classification of Atrial Fibrillation, which is the most common heart arrhythmia. We show that pretraining improves the performance of CNNs on the target task by up to \$\$6.57{\textbackslash}\%\$\$, effectively reducing the number of annotations required to achieve the same performance as CNNs that are not pretrained. We investigate both supervised as well as unsupervised pretraining approaches, which we believe will increase in relevance, since they do not rely on the expensive ECG annotations. The code is available on GitHub at https://github.com/kweimann/ecg-transfer-learning.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Machine learning},
  file = {C:\Users\armin\Zotero\storage\ZAK9LMUV\Weimann und Conrad - 2021 - Transfer learning for ECG classification.pdf}
}

@article{wickhamTestthatGetStarted2011,
  title = {Testthat: {{Get}} Started with Testing},
  author = {Wickham, Hadley},
  year = {2011},
  journal = {The R Journal},
  volume = {3},
  pages = {5--10}
}

@article{wilkinsonComparisonJointSpecies2019,
  title = {A Comparison of Joint Species Distribution Models for Presence--Absence Data},
  author = {Wilkinson, David P. and Golding, Nick and {Guillera-Arroita}, Gurutzeta and Tingley, Reid and McCarthy, Michael A.},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {2},
  pages = {198--211},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13106},
  urldate = {2024-12-18},
  abstract = {Joint species distribution models (JSDMs) account for biotic interactions and missing environmental predictors in correlative species distribution models. Several different JSDMs have been proposed in the literature, but the use of different or conflicting nomenclature and statistical notation potentially obscures similarities and differences among them. Furthermore, new JSDM implementations have been illustrated with different case studies, preventing direct comparisons of computational and statistical performance. We aim to resolve these outstanding issues by (a) highlighting similarities among seven presence--absence JSDMs using a clearly defined, singular notation; and (b) evaluating the computational and statistical performance of each JSDM using six datasets that vary widely in numbers of sites, species, and environmental covariates considered. Our singular notation shows that many of the JSDMs are very similar, and in turn parameter estimates of different JSDMs are moderate to strongly, positively correlated. In contrast, the different JSDMs clearly differ in computational efficiency and memory limitations. Our framework will allow ecologists to make educated decisions about the JSDM that best suits their objective, and enable wider uptake of JSDM methods among the ecological community.},
  copyright = {{\copyright} 2018 The Authors. Methods in Ecology and Evolution {\copyright} 2018 British Ecological Society},
  langid = {english},
  keywords = {biotic interactions,community assembly,hierarchical models,joint species distribution model,latent factors,presence-absence,residual correlation},
  file = {C\:\\Users\\armin\\Zotero\\storage\\W9PXQ6U3\\Wilkinson et al. - 2019 - A comparison of joint species distribution models .pdf;C\:\\Users\\armin\\Zotero\\storage\\DYR2997Q\\2041-210X.html}
}

@article{wrightRangerFastImplementation2017,
  title = {{{ranger}}: A Fast Implementation of Random Forests for High Dimensional Data in {{C}}++ and {{R}}},
  author = {Wright, Marvin N. and Ziegler, Andreas},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {77},
  number = {1},
  pages = {1--17},
  doi = {10.18637/jss.v077.i01}
}

@article{zhangNovelMultimodalSpecies2022,
  title = {A {{Novel Multimodal Species Distribution Model Fusing Remote Sensing Images}} and {{Environmental Features}}},
  author = {Zhang, Xiaojuan and Zhou, Yongxiu and Peng, Peihao and Wang, Guoyan},
  year = {2022},
  month = jan,
  journal = {Sustainability},
  volume = {14},
  number = {21},
  pages = {14034},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su142114034},
  urldate = {2023-05-11},
  abstract = {Species distribution models (SDMs) are critical in conservation decision-making and ecological or biogeographical inference. Accurately predicting species distribution can facilitate resource monitoring and management for sustainable regional development. Currently, species distribution models usually use a single source of information as input for the model. To determine a solution to the lack of accuracy of the species distribution model with a single information source, we propose a multimodal species distribution model that can input multiple information sources simultaneously. We used ResNet50 and Transformer network structures as the backbone for multimodal data modeling. The model's accuracy was tested using the GEOLIFE2020 dataset, and our model's accuracy is state-of-the-art (SOTA). We found that the prediction accuracy of the multimodal species distribution model with multiple data sources of remote sensing images, environmental variables, and latitude and longitude information as inputs (29.56\%) was higher than that of the model with only remote sensing images or environmental variables as inputs (25.72\% and 21.68\%, respectively). We also found that using a Transformer network structure to fuse data from multiple sources can significantly improve the accuracy of multimodal models. We present a novel multimodal model that fuses multiple sources of information as input for species distribution prediction to advance the research progress of multimodal models in the field of ecology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,feature fusion,high-resolution remote sensing images,multimodal,species distribution models,Transformer network},
  file = {C:\Users\armin\Zotero\storage\8U6FPCK8\Zhang et al. - 2022 - A Novel Multimodal Species Distribution Model Fusi.pdf}
}

@article{zhangNovelMultimodalSpecies2022a,
  title = {A {{Novel Multimodal Species Distribution Model Fusing Remote Sensing Images}} and {{Environmental Features}}},
  author = {Zhang, Xiaojuan and Zhou, Yongxiu and Peng, Peihao and Wang, Guoyan},
  year = {2022},
  month = jan,
  journal = {Sustainability},
  volume = {14},
  number = {21},
  pages = {14034},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su142114034},
  urldate = {2024-12-18},
  abstract = {Species distribution models (SDMs) are critical in conservation decision-making and ecological or biogeographical inference. Accurately predicting species distribution can facilitate resource monitoring and management for sustainable regional development. Currently, species distribution models usually use a single source of information as input for the model. To determine a solution to the lack of accuracy of the species distribution model with a single information source, we propose a multimodal species distribution model that can input multiple information sources simultaneously. We used ResNet50 and Transformer network structures as the backbone for multimodal data modeling. The model's accuracy was tested using the GEOLIFE2020 dataset, and our model's accuracy is state-of-the-art (SOTA). We found that the prediction accuracy of the multimodal species distribution model with multiple data sources of remote sensing images, environmental variables, and latitude and longitude information as inputs (29.56\%) was higher than that of the model with only remote sensing images or environmental variables as inputs (25.72\% and 21.68\%, respectively). We also found that using a Transformer network structure to fuse data from multiple sources can significantly improve the accuracy of multimodal models. We present a novel multimodal model that fuses multiple sources of information as input for species distribution prediction to advance the research progress of multimodal models in the field of ecology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,feature fusion,high-resolution remote sensing images,multimodal,species distribution models,Transformer network},
  file = {C:\Users\armin\Zotero\storage\76ZSRWUQ\Zhang et al. - 2022 - A Novel Multimodal Species Distribution Model Fusi.pdf}
}
